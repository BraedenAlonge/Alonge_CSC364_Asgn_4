
Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.


Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



Lab 6 Report
	This lab explores two analytical methods for authorship attribution of the Reuter’s 50-50 dataset– Supervised and unsupervised learning. We implemented supervised learning using K-Nearest Neighbors (KNN) classification with custom tf-idf vectorization, comparing cosine similarity and a custom Okapi similarity metric. To represent unsupervised learning, K-Means clustering was demonstrated to group articles into 50 clusters and evaluate cluster purity and other metrics.
We ran tests using different Ns in KNN, using stemming vs. not using stemming, using the different levels of stop word removal, and both cosine and Okapi similarity. We tested the methods with all 3 levels of stop word removal with and without stemming, and used the following specific hyperparameters:
Number of neighbors (k) ranging from 3 to 15
Similarity metrics: Cosine similarity and a custom Okapi similarity
Number of clusters fixed at 50 to reflect the number of authors
Maximum iterations varied from 300 to 1200
Distance metric: Cosine similarity for computing distances

1. Supervised Learning: KNN
Best hyperparameter tuning:
Medium-length stop words
Use stemming
3 neighbors
Cosine similarity metric
Accuracy:
Overall Accuracy: 0.722 (3608/5000)

2. Unsupervised Learning: K-Means Clustering
Best hyperparameter tuning:
Medium-length stop words
Use stemming
50 clusters
Cosine similarity metric
850 maximum iterations
Accuracy:
Average cluster purity: 0.52655201887397
Clustering Rand Score: 0.925980076015203

 Reflection on the Methods
KNN classification seemed to have a solid overall accuracy and authors with distinctive
 vocabularies made it easy to predict accurately. It also appears, however, that authors
  that had similar writing styles in terms of their vocabularies or had limited “rare” words
  were more difficult to classify. For K-Means clustering, it looks like the relatively high
  rand score indicates that the groupings were agreeing a fair amount with the truth labels—however,
   the average cluster purity was not very high, so clusters tended to not be super accurate alone
   in separating the similar authors from one another correctly.
In terms of specific information about authorship, some authors like FumikoFujisaki seem to be very
easy to predict. Clustering and KNN both had very high measures of success. What is interesting to us
is that authors that seem to have success in KNN will also often have a higher number of plurality clusters
 in K-means. FumikoFujisaki, for example, has an F1 score of 0.921 in KNN and has 3 plurality clusters
  from K-means. The vice versa is also true. Another author, WilliamKazer, has an F1 score of 0.385 and
  has 0 plurality clusters from K-means—his documents are actually spread out in 13 separate clusters.
  Our analysis showed that K-Means clustering showed us which authors tended to be confused with another
  . BenjaminKangLim and JaneMacartney, for example, had clusters split between them fairly commonly, which
   shows that those two authors have a similar writing style—this is also why both of those authors seem
   to do poorly in KNN, with about a 0.5 F1 score for both of them. These are just a couple examples of
    the authors that seem to be very easy or very hard to predict. The performance between KNN and K-means,
     obviously, seems fairly linked.

Comparison of the Methods
Overall, even with their similar behavior and performance, the KNN classifier (supervised method) achieved
 slightly higher overall accuracy (72.2%) compared to the clustering approach. KNN provided more consistent
  and detailed per-author performance metrics, making it more effective for precise authorship attribution.
   Even after trying many different hyperparameters for each approach, KNN seemed, on average, to produce
    slightly more accurate results over K-Means Clustering.
Based on the experiments, even though the methods are fairly similar, we believe that the supervised KNN
method proved to be more accurate and reliable for attributing authorship of the Reuters 50-50 dataset.
Nonetheless, the clustering method does offer useful insights into the “natural structure” of the dataset.
 Clustering was more useful to us to see which authors were most closely related to one another. Taking a
 look at the raw output of the clusters (but mainly the printed output after running the programs), it
  is helpful to see how each author is grouped among other authors and can lead to further investigation
   of similarities between these authors.



